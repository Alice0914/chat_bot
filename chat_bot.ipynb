{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mFRlm4MbNvf",
        "outputId": "50bbb26c-b129-4cda-fcf5-52cfc91f48df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "m0pWKxDkdI5N"
      },
      "outputs": [],
      "source": [
        "%pip install -U --quiet langchain-google-genai langchain tiktoken pypdf sentence_transformers chromadb langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Chat Bot\n",
        "\n",
        "*   Split text into sentences\n",
        "*   Duplicate first and last page texts\n",
        "*   Use Parent-Child Document Retriever"
      ],
      "metadata": {
        "id": "e9B72NhF9PDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import PyPDF2\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from markdown import Markdown\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnableMap\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed"
      ],
      "metadata": {
        "id": "GcXAOYZl9PHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a Document class to store page content and metadata\n",
        "class Document:\n",
        "    def __init__(self, page_content, metadata):\n",
        "        self.page_content = page_content\n",
        "        self.metadata = metadata\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Document(page_content={self.page_content!r}, metadata={self.metadata})\"\n",
        "\n",
        "# Define a DataLoader class to handle the loading and chunking of PDF data\n",
        "class DataLoaderParentChildChunks:\n",
        "    def __init__(self, input_file, parent_chunk_size, child_chunk_size, chunk_overlap):\n",
        "        self.input_file = input_file\n",
        "        self.parent_chunk_size = parent_chunk_size\n",
        "        self.child_chunk_size = child_chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "\n",
        "    # Get the total number of pages in the PDF\n",
        "    def get_total_pages(self):\n",
        "        with open(self.input_file, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            total_pages = len(reader.pages)\n",
        "        return total_pages\n",
        "\n",
        "    # Load a specific page from the PDF\n",
        "    def load_pdf_page(self, page_num):\n",
        "        with open(self.input_file, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            page = reader.pages[page_num]\n",
        "            page_text = page.extract_text()\n",
        "        return page_text, page_num + 1\n",
        "\n",
        "    # Split text into sentences using regular expressions\n",
        "    def split_into_sentences(self, text):\n",
        "        sentence_endings = re.compile(r'(?<=[.!?])\\s+(?=[A-Z])')\n",
        "        return sentence_endings.split(text)\n",
        "\n",
        "    # Create chunks from sentences with specified chunk size and overlap\n",
        "    def create_chunks(self, sentences, page_numbers, chunk_size):\n",
        "        chunks = []\n",
        "        num_sentences = len(sentences)\n",
        "        step = chunk_size - self.chunk_overlap\n",
        "        for i in range(0, num_sentences, step):\n",
        "            chunk_sentences = sentences[i:i + chunk_size]\n",
        "            chunk_pages = page_numbers[i:i + chunk_size]\n",
        "            chunk = ' '.join(chunk_sentences)\n",
        "            if chunk:\n",
        "                chunks.append({\n",
        "                    'Text': chunk,\n",
        "                    'Source': self.input_file,\n",
        "                    'Page': ', '.join(map(str, sorted(set(chunk_pages))))\n",
        "                })\n",
        "        return chunks\n",
        "\n",
        "    # Main function to load PDF, split into sentences, and create chunks\n",
        "    def run(self, num_pages=None):\n",
        "        total_pages = self.get_total_pages()\n",
        "        if num_pages is None:\n",
        "            num_pages = total_pages\n",
        "\n",
        "        combined_text = \"\"\n",
        "        page_texts = []\n",
        "\n",
        "        # Use ThreadPoolExecutor to load pages in parallel\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            future_to_page = {executor.submit(self.load_pdf_page, page_num): page_num for page_num in range(min(num_pages, total_pages))}\n",
        "            for future in as_completed(future_to_page):\n",
        "                page_text, page_num = future.result()\n",
        "                page_texts.append((page_text, page_num))\n",
        "                combined_text += page_text + \" \"\n",
        "\n",
        "        # Add first and last page texts again for context\n",
        "        if total_pages > 0:\n",
        "            first_page_text = self.load_pdf_page(0)[0]\n",
        "            last_page_text = self.load_pdf_page(total_pages - 1)[0]\n",
        "            page_texts.insert(1, (first_page_text, 1))\n",
        "            page_texts.append((last_page_text, total_pages))\n",
        "            combined_text = first_page_text + \" \" + combined_text + \" \" + last_page_text\n",
        "\n",
        "        # Split combined text into sentences and track page numbers\n",
        "        sentences = []\n",
        "        page_numbers = []\n",
        "        for page_text, page_num in page_texts:\n",
        "            page_sentences = self.split_into_sentences(page_text)\n",
        "            sentences.extend(page_sentences)\n",
        "            page_numbers.extend([page_num] * len(page_sentences))\n",
        "\n",
        "        # Create parentchunks and child chunks\n",
        "        parent_chunks = self.create_chunks(sentences, page_numbers, self.parent_chunk_size)\n",
        "        child_chunks = self.create_chunks(sentences, page_numbers, self.child_chunk_size)\n",
        "\n",
        "        # Create document texts from the child chunks\n",
        "        child_documents = []\n",
        "        for idx, row in pd.DataFrame(child_chunks).iterrows():\n",
        "            page_content = row['Text']\n",
        "            metadata = {'source': row['Source'], 'page': row['Page']}\n",
        "            child_documents.append(Document(page_content=page_content, metadata=metadata))\n",
        "\n",
        "        # Create document texts from the parent chunks\n",
        "        parent_documents = []\n",
        "        for idx, row in pd.DataFrame(parent_chunks).iterrows():\n",
        "            page_content = row['Text']\n",
        "            metadata = {'source': row['Source'], 'page': row['Page']}\n",
        "            parent_documents.append(Document(page_content=page_content, metadata=metadata))\n",
        "\n",
        "        return child_documents, parent_documents"
      ],
      "metadata": {
        "id": "k3uLdAXCMIlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def question_answering(quest):\n",
        "    # Mount Google Drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = \"Your Google API\"\n",
        "\n",
        "    # Load and split PDF using DataLoaderParentChildChunks\n",
        "    pdf_path = \"/content/drive/MyDrive/Colab Notebooks/GenAI_Handbook.pdf\"\n",
        "    loader = DataLoaderParentChildChunks(pdf_path, parent_chunk_size=500, child_chunk_size=200, chunk_overlap=100)\n",
        "    child_documents, parent_documents = loader.run()\n",
        "\n",
        "    # Initialize HuggingFace Embeddings\n",
        "    model_name = \"all-mpnet-base-v2\"\n",
        "    hf = HuggingFaceEmbeddings(\n",
        "        model_name=model_name,\n",
        "        model_kwargs={'device': 'cpu'},\n",
        "        encode_kwargs={'normalize_embeddings': True}\n",
        "    )\n",
        "\n",
        "    # Create a document search index and save embeddings in vector DB\n",
        "    child_docsearch = Chroma.from_documents(child_documents, hf)\n",
        "    parent_docsearch = Chroma.from_documents(parent_documents, hf)\n",
        "\n",
        "    # Configure the retrieve\n",
        "    retriever = child_docsearch.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={\"k\": 5, \"fetch_k\": 152}\n",
        "    )\n",
        "\n",
        "    template = \"\"\"Answer the question based only on the following context:\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "    \"\"\"\n",
        "    prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "    gemini = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0)\n",
        "\n",
        "    chain = RunnableMap({\n",
        "        \"context\": lambda x: retriever.get_relevant_documents(x['question']),\n",
        "        \"question\": lambda x: x['question']\n",
        "    }) | prompt | gemini\n",
        "\n",
        "    response = chain.invoke({'question': quest})\n",
        "\n",
        "    # Find the parent chunks related to the retrieved child chunks\n",
        "    relevant_child_chunks = retriever.get_relevant_documents(quest)\n",
        "    relevant_parent_chunks = []\n",
        "    for child_chunk in relevant_child_chunks:\n",
        "        parent_docs = parent_docsearch.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={\"k\": 5}\n",
        "        ).get_relevant_documents(child_chunk.page_content)\n",
        "        relevant_parent_chunks.extend(parent_docs)\n",
        "\n",
        "    # Ensure relevant_parent_chunks is unique\n",
        "    unique_parent_chunks = {doc.page_content: doc for doc in relevant_parent_chunks}.values()\n",
        "\n",
        "    # Update the context in the template with the unique parent chunks\n",
        "    parent_context = ' '.join([doc.page_content for doc in unique_parent_chunks])\n",
        "    template = template.replace(\"{context}\", parent_context)\n",
        "\n",
        "    response = chain.invoke({'question': quest})\n",
        "    return response.content"
      ],
      "metadata": {
        "id": "Qn4Ga_p3MIhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quest = \"What are some chunking strategies??\"\n",
        "question_answering(quest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "R6csi4OXHUCe",
        "outputId": "a11747ca-7f04-4930-fbe0-3c951bc8b328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'- Fixed-size chunking\\n- Recursive chunking\\n- Parent-Child Document Retriever'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quest = \"what is Parent-Child Document Retriever?\"\n",
        "question_answering(quest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "VwRsyToBG7gI",
        "outputId": "ed585bba-e768-4366-d37c-dff71ef038a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Parent-Child Document Retriever is a technique used to address the challenge of creating chunks of data that are both small enough to reduce noise and large enough to provide sufficient context for a language model (LLM). It involves creating two chunk sizes and two chunk overlaps: one for large chunks and one for small chunks. The original data is first split into large chunks, and then the large chunks are split into small chunks. The small chunks contain a reference to the large chunks they were derived from. The small chunks are used to create vector embeddings, which are used during similarity search. The large chunks are used to provide the necessary context to the LLM for it to generate text.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quest = \"What are the two common chunking parameters?\"\n",
        "question_answering(quest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "nuzSkh6KHEUs",
        "outputId": "4f7e1323-882c-44f4-de3b-5b731fa9377b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chunk size and chunk overlap'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quest = \"What is prompt management?\"\n",
        "question_answering(quest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "_fw8aZxDGF73",
        "outputId": "17ba01ab-cf81-4289-ac6c-53c9b1bdcec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Prompt management is essential to ensure developers can evaluate multiple prompts.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quest = \"How should users develop with LangChain?\"\n",
        "question_answering(quest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "-T1-GdWxGF4t",
        "outputId": "6ec1e9a9-3a04-4f79-b8fc-2e9d7db47726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'When developing with the LangChain framework, it is recommended to pull down and extend the larger, more complex implementations. This is most notably the core classes (ex: AzureOpenAI, Agent). After pulling down the code from the relevant API version, developers can modify it for their use case. This simplifies extending LangChain’s existing classes as well as the debugging process. This method can also be used to incorporate logging and extra error handling.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quest = \"What are evaluation framework?\"\n",
        "question_answering(quest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "oRywUP9MGF1z",
        "outputId": "c102fb95-9294-4c8b-e6cb-40c58b3d6b2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'An evaluation framework is essential to efficiently compare LLM architectures, specifically architecture approaches and all the parameters involved. The framework defines specific key evaluation metrics and a process for calculating these metrics using variable prompts.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hJg9tuPnGFvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wRWL4xvQGFr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s4qbfRjrCCOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gXhknYhRCCLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tgyE9iI_CCIR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}